# streamlit_app.py â€” RQç”Ÿæˆï¼ˆé–¢å¿ƒé ˜åŸŸè¦–ç‚¹è¾¼ã¿ï¼‰ï¼‹CSVå‡ºåŠ›ï¼‹è«–æ–‡ãƒªãƒ³ã‚¯
import streamlit as st
import requests
import pandas as pd
from urllib.parse import quote
from openai import OpenAI

st.title("ğŸ“ Research Question Generator")
st.write(
    "ãƒ‘ãƒãƒ«/ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼è¦ç´„ã‹ã‚‰4è¦–ç‚¹ï¼ˆé€†å¼µã‚Š/é£›ã°ã—/ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•å¹»åƒ/ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼ï¼‰ã§ç ”ç©¶ã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³ã‚’ç”Ÿæˆã—ã€"
    "ã”é–¢å¿ƒé ˜åŸŸã®è¦³ç‚¹ï¼ˆEntrepreneurship & Innovation / VC & Entrepreneurial Finance / Public Policy & Institutional Design / "
    "Applied Econometrics / Cross-border Investmentï¼‰ã‚’ä»˜ä¸ã€‚æ–°è¦æ€§Ã—å®Ÿç”¨æ€§ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ã¦CSVã‚’å‡ºåŠ›ã—ã¾ã™ã€‚"
)

client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# ========= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
@st.cache_data(show_spinner=False, ttl=600)
def openalex_count(query: str) -> int:
    url = f"https://api.openalex.org/works?search={quote(query)}&per_page=1"
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
        return int(r.json().get("meta", {}).get("count", 0))
    except Exception:
        return -1  # ä¸æ˜

@st.cache_data(show_spinner=False, ttl=600)
def openalex_top_links(query: str, n: int = 3) -> list[dict]:
    """é–¢é€£ä¸Šä½è«–æ–‡ã®ãƒªãƒ³ã‚¯ã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¿”ã™ï¼ˆOpenAlex/DOIï¼‰"""
    url = f"https://api.openalex.org/works?search={quote(query)}&per_page={n}&sort=cited_by_count:desc"
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
        items = r.json().get("results", [])
        out = []
        for it in items:
            oid = it.get("id", "")  # e.g., https://openalex.org/W123...
            title = (it.get("title") or "").strip()
            doi = (it.get("doi") or "").strip()  # e.g., https://doi.org/...
            olink = oid if oid else ""
            dlink = doi if doi else ""
            # è¡¨ç¤ºã¯ã€ŒTitle | OpenAlex | DOI(ã‚ã‚Œã°)ã€
            disp = title
            links = [olink] + ([dlink] if dlink else [])
            out.append({"title": disp, "links": " | ".join([l for l in links if l])})
        return out
    except Exception:
        return []

def novelty_score_from_count(n: int) -> int:
    if n < 0:   return 2
    if n < 50:  return 5
    if n < 150: return 4
    if n < 400: return 3
    if n < 1000:return 2
    return 1

def openalex_search_url(query: str) -> str:
    return f"https://api.openalex.org/works?search={quote(query)}"

def ask_gpt_utility(q: str, context: str) -> dict:
    prompt = f"""
ä»¥ä¸‹ã®ç ”ç©¶ã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³ã«ã¤ã„ã¦ã€ãƒ™ãƒ³ãƒãƒ£ãƒ¼æŠ•è³‡å®¶ãƒ»æ”¿ç­–ç«‹æ¡ˆè€…ã®å®Ÿå‹™ã«ã¨ã£ã¦ã®æœ‰ç”¨æ€§ã‚’5ç‚¹æº€ç‚¹ã§è©•ä¾¡ã—ã€
çŸ­ã„ç†ç”±ã‚’1ã€œ2æ–‡ã§è¿°ã¹ã¦ãã ã•ã„ã€‚JSONã§è¿”ã—ã¦ãã ã•ã„ï¼ˆkeys: score, reasonï¼‰ã€‚

[ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ]
{context}

[ç ”ç©¶ã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³]
{q}
"""
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
    )
    txt = resp.choices[0].message.content.strip()
    import json, re
    try:
        m = re.search(r"\{.*\}", txt, re.S)
        data = json.loads(m.group(0)) if m else {}
        score = int(data.get("score", 3))
        reason = str(data.get("reason", "")).strip()[:200]
    except Exception:
        score, reason = 3, txt[:200]
    score = max(1, min(5, score))
    return {"score": score, "reason": reason}

def ask_gpt_perspective_tags(q: str) -> list[str]:
    """é–¢å¿ƒé ˜åŸŸã«ç…§ã‚‰ã—ã¦ã©ã®è¦–ç‚¹ãŒå¼·ã„ã‹ã‚¿ã‚°ä»˜ã‘"""
    prompt = f"""
æ¬¡ã®ç ”ç©¶ã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®é–¢å¿ƒé ˜åŸŸã®ã†ã¡è©²å½“ã™ã‚‹ã‚‚ã®ã‚’1ã€œ3å€‹ã€çŸ­ã„è‹±èªã‚¿ã‚°ã§è¿”ã—ã¦ãã ã•ã„ã€‚
è¿”ç­”ã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ã‚¿ã‚°ã®ã¿ï¼ˆèª¬æ˜ä¸è¦ï¼‰ã€‚

- Entrepreneurship & Innovation
- Venture Capital & Entrepreneurial Finance
- Public Policy & Institutional Design
- Applied Econometrics
- Cross-border Investment

Question: {q}
"""
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
    )
    tags = [t.strip() for t in resp.choices[0].message.content.split(",") if t.strip()]
    return tags[:3]

def generate_rqs(context: str) -> dict:
    """4ãƒ•ãƒ¬ãƒ¼ãƒ Ã—å„1å•ã€‚é–¢å¿ƒé ˜åŸŸã‚’å‰ç½®ã—ã¦ç”Ÿæˆã‚’èª˜å°ã€‚"""
    prompt = f"""
ã‚ãªãŸã¯PhDç ”ç©¶æ”¯æ´ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®é–¢å¿ƒé ˜åŸŸã®è¦³ç‚¹ã‚’å¸¸ã«æ„è­˜ã—ã¦ã€è­°è«–è¦ç´„ã‹ã‚‰ç ”ç©¶ã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³ã‚’ä½œã£ã¦ãã ã•ã„ï¼š
- Entrepreneurship & Innovation
- Venture Capital & Entrepreneurial Finance
- Public Policy & Institutional Design
- Applied Econometrics
- Cross-border Investment

4ã¤ã®ç™ºæƒ³ãƒ•ãƒ¬ãƒ¼ãƒ ã§å„1å•ãšã¤ã€æ—¥æœ¬èªã§ç°¡æ½”ã«æç¤ºï¼š
1. é€†å¼µã‚Šï¼ˆå‰æã‚’é€†ã«è¦‹ã‚‹ï¼‰
2. é£›ã°ã—ï¼ˆæ‰‹æ®µBã‚’å‰æã¨ã›ãšAã‚’é”æˆã™ã‚‹æ–¹æ³•ï¼‰
3. ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®å¹»æƒ³ï¼ˆAã¨Bã‚’åŒæ™‚é”æˆã§ãã‚‹æ¡ä»¶ï¼‰
4. ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼ï¼ˆä»–åˆ†é‡ã¸ã®è»¢ç”¨ï¼‰

å‡ºåŠ›å½¢å¼ï¼šå„è¡Œã€Œ<ãƒ•ãƒ¬ãƒ¼ãƒ >ï¼š<ã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³>ã€ã®ã¿ï¼ˆèª¬æ˜æ–‡ãªã—ï¼‰ã€‚

[è­°è«–è¦ç´„]
{context}
"""
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5,
    )
    out = resp.choices[0].message.content.strip()
    lines = [l.strip("- ").strip() for l in out.splitlines() if l.strip()]
    buckets = {"é€†å¼µã‚Š": "", "é£›ã°ã—": "", "ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®å¹»æƒ³": "", "ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼": ""}
    for k in list(buckets.keys()):
        for l in lines:
            if l.startswith(k):
                q = l.split("ï¼š", 1)[-1].split(":", 1)[-1].strip()
                buckets[k] = q or l
                break
    i = 0
    for k in list(buckets.keys()):
        if not buckets[k] and i < len(lines):
            buckets[k] = lines[i]
            i += 1
    return buckets

# ========= UI =========
colA, colB = st.columns([3, 2])
with colB:
    st.markdown("**ã‚¹ã‚³ã‚¢é‡ã¿**")
    w_n = st.slider("æ–°è¦æ€§ã®é‡ã¿", 0.0, 1.0, 0.6, 0.1)
    w_u = 1.0 - w_n
    st.caption(f"ç·åˆç‚¹ = æ–°è¦æ€§Ã—{w_n:.1f} + å®Ÿç”¨æ€§Ã—{w_u:.1f}")
with colA:
    summary = st.text_area("è­°è«–ã®è¦ç´„ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", height=200)

if st.button("ç”Ÿæˆ & ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆCSVå‡ºåŠ›ã¤ãï¼‰"):
    if not summary.strip():
        st.warning("å…ˆã«è¦ç´„ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("ç ”ç©¶ã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³ã‚’ç”Ÿæˆä¸­..."):
            rqs = generate_rqs(summary)

        rows = []
        with st.spinner("ã‚¹ã‚³ã‚¢ç®—å‡ºãƒ»ãƒªãƒ³ã‚¯åé›†ä¸­..."):
            for frame, q in rqs.items():
                base_query = q if len(q) > 10 else (summary + " " + q)

                # æ–°è¦æ€§ï¼ˆOpenAlexä»¶æ•°ï¼‰
                count = openalex_count(base_query)
                nov = novelty_score_from_count(count)
                search_url = openalex_search_url(base_query)

                # ä¸Šä½è«–æ–‡ãƒªãƒ³ã‚¯ï¼ˆOpenAlex / DOIï¼‰
                top = openalex_top_links(base_query, n=3)
                top_links = "; ".join([f"{t['title']} | {t['links']}" for t in top]) if top else ""

                # å®Ÿç”¨æ€§ï¼ˆLLMï¼‰
                util = ask_gpt_utility(q, summary)

                # é–¢å¿ƒé ˜åŸŸã‚¿ã‚°ï¼ˆLLMï¼‰
                tags = ask_gpt_perspective_tags(q)

                score = round(nov * w_n + util["score"] * w_u, 2)

                rows.append({
                    "ç™ºæƒ³ãƒ•ãƒ¬ãƒ¼ãƒ ": frame,
                    "ç ”ç©¶ã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³": q,
                    "é–¢å¿ƒé ˜åŸŸã‚¿ã‚°": ", ".join(tags),
                    "æ–°è¦æ€§(1-5)": nov,
                    "å®Ÿç”¨æ€§(1-5)": util["score"],
                    "ç·åˆã‚¹ã‚³ã‚¢": score,
                    "å®Ÿç”¨æ€§ã‚³ãƒ¡ãƒ³ãƒˆ": util["reason"],
                    "OpenAlexä»¶æ•°(ç›®å®‰)": count if count >= 0 else "N/A",
                    "OpenAlexæ¤œç´¢URL": search_url,
                    "é–¢é€£ä¸Šä½è«–æ–‡": top_links,  # ã€ŒTitle | OpenAlex | DOIã€
                })

        df = pd.DataFrame(rows).sort_values("ç·åˆã‚¹ã‚³ã‚¢", ascending=False).reset_index(drop=True)
        st.subheader("ğŸ ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆCSVã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ï¼‰")
        st.dataframe(df, use_container_width=True)

        csv = df.to_csv(index=False).encode("utf-8")
        st.download_button("â¬‡ï¸ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name="rq_ranked_with_links.csv", mime="text/csv")
