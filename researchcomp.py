import os
import time
import json
import math
import requests
import pandas as pd
import streamlit as st
from typing import List, Dict, Any
from urllib.parse import urlencode

# =========================
# Ë®≠ÂÆöÔºàSecrets „ÇíÂÑ™ÂÖàÔºâ
# =========================
GOOGLE_API_KEY = st.secrets.get("GOOGLE_API_KEY", os.getenv("GOOGLE_API_KEY", ""))
GOOGLE_CSE_ID  = st.secrets.get("GOOGLE_CSE_ID",  os.getenv("GOOGLE_CSE_ID",  ""))
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY", ""))

# =========================
# OpenAI (Responses API)
# =========================
# refs: client.responses.create / Python SDKÔºàÂÖ¨ÂºèÔºâ
from openai import OpenAI  # pip install openai>=1.0.0
_oai = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

def ask_openai_reasoning(company: str, evidence: Dict[str, List[Dict[str, str]]]) -> Dict[str, Any]:
    """
    evidence „ÅØÂêÑ„Ç´„ÉÜ„Ç¥„É™„Éº„Åî„Å®„Å´ [{'title','link','snippet'} ...] „ÅÆÈÖçÂàó
    Ëøî„ÇäÂÄ§: „Ç´„ÉÜ„Ç¥„É™„Åî„Å®„ÅÆ {label, confidence, reason_ja, reason_en} „Å®„ÄÅË¶ÅÁ¥Ñ„ÉÑ„Ç§„Éº„Éà jp/en
    """
    if not _oai:
        return {}

    # 1Á§æ„Åî„Å®„Éó„É≠„É≥„Éó„ÉàÔºàÊó•Êú¨Ë™û„ÅßÊåáÁ§∫ÔºãËã±Ë™ûÂá∫Âäõ„ÇÇË¶ÅÊ±ÇÔºâ
    sys = (
        "You are an analyst for corporate‚Äìstartup collaboration. "
        "Return JSON only. Be concise, cite evidence URLs explicitly."
    )

    user = {
        "company": company,
        "tasks": [
            "CVC„ÇíÁ´ã„Å°‰∏ä„Åí„Å¶„ÅÑ„Çã„Åã (CVC)",
            "LPÊäïË≥á„Çí„Åó„Å¶„ÅÑ„Çã„Åã (LP)",
            "AI/Robotics„Å®‰∫ãÊ•≠„Ç∑„Éä„Ç∏„Éº (AI_Robotics)",
            "Healthcare„Å®‰∫ãÊ•≠„Ç∑„Éä„Ç∏„Éº (Healthcare)",
            "Climate tech„Å®‰∫ãÊ•≠„Ç∑„Éä„Ç∏„Éº (Climate)"
        ],
        "instruction": (
            "ÂêÑ„Çø„Çπ„ÇØ„Å´„Å§„ÅÑ„Å¶: label „Çí 'Yes' | 'No' | 'Unclear' „Åã„Çâ„ÄÅconfidence „Çí 0-1„ÄÅ"
            "reason_ja „ÇíÊó•Êú¨Ë™û100Â≠ó‰ª•ÂÜÖ„ÄÅreason_en „ÇíËã±Ë™û„Åß1-2Êñá„ÄÇ"
            "ÂøÖ„ÅöÊ†πÊã†URL(Ë¶ã„Å§„Åã„Å£„ÅüÁØÑÂõ≤„ÅßÊúÄÂ§ß3‰ª∂)„Çí evidence_urls „Å´Âê´„ÇÅ„Çã„ÄÇ"
            "ÊúÄÂæå„Å´ X ÊäïÁ®ø„Éâ„É©„Éï„Éà: jp „ÅØÂÖ®Ëßí140Â≠ó‰ª•ÂÜÖÔºàURL„Å™„Åó„ÉªÊú¨Êñá„ÅÆ„ÅøÔºâ„ÄÅen „ÅØËã±Ë™û280ÊñáÂ≠ó‰ª•ÂÜÖÔºàÊúÄÈáçË¶ÅURL„Çí1„Å§„Å†„ÅëÊú´Â∞æ„Å´Ôºâ„ÄÇ"
            "Âá∫Âäõ„ÅØ‰ª•‰∏ãJSON„Çπ„Ç≠„Éº„Éû„Å´Âé≥ÂØÜÊ∫ñÊã†:\n"
            "{"
            "  'per_task': {"
            "     'CVC':        {'label':'', 'confidence':0.0, 'reason_ja':'', 'reason_en':'', 'evidence_urls':[]},"
            "     'LP':         {'label':'', 'confidence':0.0, 'reason_ja':'', 'reason_en':'', 'evidence_urls':[]},"
            "     'AI_Robotics':{'label':'', 'confidence':0.0, 'reason_ja':'', 'reason_en':'', 'evidence_urls':[]},"
            "     'Healthcare': {'label':'', 'confidence':0.0, 'reason_ja':'', 'reason_en':'', 'evidence_urls':[]},"
            "     'Climate':    {'label':'', 'confidence':0.0, 'reason_ja':'', 'reason_en':'', 'evidence_urls':[]}"
            "  },"
            "  'x_post': {'jp':'', 'en':''}"
            "}"
        ),
        "evidence": evidence
    }

    # Responses API
    resp = _oai.responses.create(
        model="gpt-4.1-mini",
        input=f"System:\n{sys}\n\nUser:\n{json.dumps(user, ensure_ascii=False)}",
        temperature=0.2,
        max_output_tokens=1200,
    )
    text = resp.output_text  # unified text out
    try:
        data = json.loads(text)
    except Exception:
        # ‰∏á„Åå‰∏ÄJSON„ÅßËøî„Çâ„Å™„ÅÑÂ†¥Âêà„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºàÁ∞°ÊòìÔºâ
        data = {"per_task": {}, "x_post": {"jp": "", "en": ""}}

    return data

# =========================
# Google Custom Search
# =========================
def google_search(q: str, num: int = 5) -> List[Dict[str, str]]:
    if not GOOGLE_API_KEY or not GOOGLE_CSE_ID:
        return []
    params = {
        "key": GOOGLE_API_KEY,
        "cx": GOOGLE_CSE_ID,
        "q": q,
        "num": min(num, 10),
        "hl": "ja",
        "gl": "jp",
        "safe": "off",
    }
    url = f"https://www.googleapis.com/customsearch/v1?{urlencode(params)}"
    r = requests.get(url, timeout=20)
    r.raise_for_status()
    j = r.json()
    items = []
    for it in j.get("items", []):
        items.append({
            "title": it.get("title",""),
            "link": it.get("link",""),
            "snippet": it.get("snippet",""),
        })
    return items

def gather_evidence(company: str) -> Dict[str, List[Dict[str, str]]]:
    queries = {
        "CVC":        f"{company} CVC „Éô„É≥„ÉÅ„É£„ÉºÊäïË≥á „Ç≥„Éº„Éù„É¨„Éº„Éà„Éô„É≥„ÉÅ„É£„Éº„Ç≠„É£„Éî„Çø„É´",
        "LP":         f"{company} LP Âá∫Ë≥á „Éô„É≥„ÉÅ„É£„Éº„Éï„Ç°„É≥„Éâ „É™„Éü„ÉÜ„ÉÉ„Éâ„Éë„Éº„Éà„Éä„Éº",
        "AI_Robotics":f"{company} AI „É≠„Éú„ÉÜ„Ç£„ÇØ„Çπ ‰∫ãÊ•≠ ÊèêÊê∫ „Çπ„Çø„Éº„Éà„Ç¢„ÉÉ„Éó",
        "Healthcare": f"{company} „Éò„É´„Çπ„Ç±„Ç¢ ÂåªÁôÇ „Éá„Ç∏„Çø„É´„Éò„É´„Çπ ÊèêÊê∫ „Çπ„Çø„Éº„Éà„Ç¢„ÉÉ„Éó",
        "Climate":    f"{company} ËÑ±ÁÇ≠Á¥† „ÇØ„É©„Ç§„É°„Éº„Éà„ÉÜ„ÉÉ„ÇØ ÂÜçÁîüÂèØËÉΩ„Ç®„Éç„É´„ÇÆ„Éº ÊèêÊê∫"
    }
    ev = {}
    for k, q in queries.items():
        time.sleep(0.3)  # „É¨„Éº„ÉàÁ∑©Âíå
        ev[k] = google_search(q, num=6)
    return ev

# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title="Corporate‚ÄìStartup Fit Checker (w/ OpenAI reasons)", layout="wide")
st.title("üè¢‚û°Ô∏èü§ùüöÄ Corporate‚ÄìStartup Fit Checker")
st.caption("Excel„ÅÆCÂàó„Å´‰ºöÁ§æÂêç„ÄÇGoogle CSE„ÅßË®ºË∑°„ÇíÈõÜ„ÇÅ„ÄÅOpenAI„ÅßÂà§ÂÆöÁêÜÁî±„Å®XÂêë„ÅëË¶ÅÁ¥Ñ„ÇíÁîüÊàê„Åó„Åæ„Åô„ÄÇ")

with st.expander("üîß SecretsË®≠ÂÆöÔºàÂøÖÈ†àÔºâ", expanded=False):
    st.markdown(
        "- `GOOGLE_API_KEY`, `GOOGLE_CSE_ID`, `OPENAI_API_KEY` „Çí **.streamlit/secrets.toml** „Å´Ë®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n"
        "```toml\n[general]\n# ‰æã:\nGOOGLE_API_KEY = \"xxxxx\"\nGOOGLE_CSE_ID  = \"xxxx:yyyy\"\nOPENAI_API_KEY = \"sk-...\"\n```\n"
    )

uploaded = st.file_uploader("Excel „Çí„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÔºàCÂàó=‰ºöÁ§æÂêçÔºâ", type=["xlsx", "xls"])
limit = st.number_input("Âá¶ÁêÜ‰ª∂Êï∞„ÅÆ‰∏äÈôêÔºà„ÉÜ„Çπ„ÉàÁî®Ôºâ", 1, 5000, 50, 10)

run = st.button("Ëß£Êûê„Çπ„Çø„Éº„Éà", type="primary", disabled=uploaded is None)

if run and uploaded:
    df = pd.read_excel(uploaded)
    # ‰ºöÁ§æÂàó„ÅÆÊé®ÂÆöÔºàCÂàóÂÑ™ÂÖàÔºâ
    if df.shape[1] >= 3:
        companies = df.iloc[:, 2].dropna().astype(str).tolist()
    else:
        companies = df.iloc[:, -1].dropna().astype(str).tolist()
    companies = companies[:int(limit)]

    rows = []
    progress = st.progress(0.0)
    status = st.empty()

    for i, company in enumerate(companies, 1):
        status.info(f"Searching: {company}")
        evidence = gather_evidence(company)

        # OpenAI„ÅßÁêÜÁî±ÁîüÊàê
        reasoning = ask_openai_reasoning(company, evidence) if OPENAI_API_KEY else {}

        per_task = reasoning.get("per_task", {})
        x_post = reasoning.get("x_post", {"jp":"", "en":""})

        def cell(task: str, field: str, default=""):
            return per_task.get(task, {}).get(field, default)

        rows.append({
            "company": company,
            # „É©„Éô„É´
            "CVC":        cell("CVC", "label", "Unclear"),
            "LP":         cell("LP", "label", "Unclear"),
            "AI_Robotics":cell("AI_Robotics", "label", "Unclear"),
            "Healthcare": cell("Healthcare", "label", "Unclear"),
            "Climate":    cell("Climate", "label", "Unclear"),
            # ‰ø°È†ºÂ∫¶
            "CVC_conf":        cell("CVC", "confidence", ""),
            "LP_conf":         cell("LP", "confidence", ""),
            "AI_Robotics_conf":cell("AI_Robotics", "confidence", ""),
            "Healthcare_conf": cell("Healthcare", "confidence", ""),
            "Climate_conf":    cell("Climate", "confidence", ""),
            # ÁêÜÁî±ÔºàÊó•/Ëã±Ôºâ
            "CVC_reason_ja":        cell("CVC", "reason_ja", ""),
            "LP_reason_ja":         cell("LP", "reason_ja", ""),
            "AI_Robotics_reason_ja":cell("AI_Robotics", "reason_ja", ""),
            "Healthcare_reason_ja": cell("Healthcare", "reason_ja", ""),
            "Climate_reason_ja":    cell("Climate", "reason_ja", ""),
            "CVC_reason_en":        cell("CVC", "reason_en", ""),
            "LP_reason_en":         cell("LP", "reason_en", ""),
            "AI_Robotics_reason_en":cell("AI_Robotics", "reason_en", ""),
            "Healthcare_reason_en": cell("Healthcare", "reason_en", ""),
            "Climate_reason_en":    cell("Climate", "reason_en", ""),
            # URLÔºàÊúÄÂ§ß3‰ª∂„Çí;Âå∫Âàá„Çä„ÅßÔºâ
            "CVC_urls":        "; ".join(per_task.get("CVC", {}).get("evidence_urls", [])),
            "LP_urls":         "; ".join(per_task.get("LP", {}).get("evidence_urls", [])),
            "AI_Robotics_urls":"; ".join(per_task.get("AI_Robotics", {}).get("evidence_urls", [])),
            "Healthcare_urls": "; ".join(per_task.get("Healthcare", {}).get("evidence_urls", [])),
            "Climate_urls":    "; ".join(per_task.get("Climate", {}).get("evidence_urls", [])),
            # XÊäïÁ®ø„Éâ„É©„Éï„Éà
            "x_post_jp": x_post.get("jp", ""),
            "x_post_en": x_post.get("en", "")
        })

        progress.progress(i/len(companies))
        time.sleep(0.05)

    out = pd.DataFrame(rows)
    st.success("ÂÆå‰∫ÜÔºÅ")
    st.dataframe(out, use_container_width=True)

    csv = out.to_csv(index=False)
    st.download_button("CSV„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ", data=csv, file_name="corporate_fit_with_reasons.csv", mime="text/csv")
