# app.py
import os
import re
import time
import json
import html
import base64
import traceback
import unicodedata
import io
import requests
import pandas as pd
import streamlit as st
from urllib.parse import urlencode, urlparse
from typing import List, Dict, Any, Tuple
from collections import defaultdict, Counter

# =========================
# è¨­å®šï¼ˆSecrets ã‚’å„ªå…ˆã€‚UIã«ã¯å‡ºã•ãªã„ï¼‰
# =========================
GOOGLE_API_KEY = st.secrets.get("GOOGLE_API_KEY", os.getenv("GOOGLE_API_KEY", ""))
GOOGLE_CSE_ID  = st.secrets.get("GOOGLE_CSE_ID",  os.getenv("GOOGLE_CSE_ID",  ""))
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY", ""))

# =========================
# OpenAI (Responses API)
# =========================
from openai import OpenAI  # pip install openai>=1.0.0
_oai = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None
MODEL_REASON = os.getenv("OPENAI_REASONING_MODEL", "gpt-4.1-mini")

# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
SAFE_HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; CorporateStartupFit/1.3)"}
REQUEST_TIMEOUT = 20

def _strip_html(raw: str) -> str:
    if not raw:
        return ""
    raw = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", raw)
    text = re.sub(r"(?s)<[^>]+>", " ", raw)
    text = html.unescape(text)
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r"[ \t\r\f\v]+", " ", text)
    text = re.sub(r"\n+", "\n", text)
    return text.strip()

def _domain_score(url: str) -> int:
    host = urlparse(url).netloc.lower()
    score = 0
    if host.endswith(".co.jp") or host.endswith(".com"): score += 1
    if any(k in host for k in ["ir.", "prtimes.jp", "prtimes.co.jp", "news.", "press"]): score += 2
    if any(p in url.lower() for p in ["/ir", "/investor", "/press", "/news", "/release"]): score += 2
    return score

def _dedup_urls(items: List[Dict[str, str]], max_per_domain: int = 3) -> List[Dict[str, str]]:
    seen = set()
    domain_counter = defaultdict(int)
    out = []
    for it in items:
        u = it.get("link", "")
        if not u or u in seen: continue
        d = urlparse(u).netloc
        if domain_counter[d] >= max_per_domain: continue
        seen.add(u); domain_counter[d] += 1; out.append(it)
    return out

# =========================
# Google Custom Search
# =========================
@st.cache_data(show_spinner=False, ttl=60*60)
def google_search(q: str, num: int = 6) -> List[Dict[str, str]]:
    if not GOOGLE_API_KEY or not GOOGLE_CSE_ID:
        return []
    params = {
        "key": GOOGLE_API_KEY,
        "cx": GOOGLE_CSE_ID,
        "q": q,
        "num": min(num, 10),
        "hl": "ja",
        "gl": "jp",
        "safe": "off",
    }
    url = f"https://www.googleapis.com/customsearch/v1?{urlencode(params)}"
    r = requests.get(url, timeout=REQUEST_TIMEOUT)
    r.raise_for_status()
    j = r.json()
    items = [{"title": it.get("title",""), "link": it.get("link",""), "snippet": it.get("snippet","")} for it in j.get("items", [])]
    items.sort(key=lambda x: _domain_score(x["link"]), reverse=True)
    return _dedup_urls(items, max_per_domain=2)

@st.cache_data(show_spinner=False, ttl=60*60)
def fetch_text(url: str, max_chars: int = 5000) -> str:
    try:
        r = requests.get(url, headers=SAFE_HEADERS, timeout=REQUEST_TIMEOUT)
        r.raise_for_status()
        text = _strip_html(r.text)
        text = re.sub(r"(?i)ã“ã®è¨˜äº‹|é–¢é€£è¨˜äº‹|ãŠã™ã™ã‚|ã‚·ã‚§ã‚¢|åŒç¤¾|ç·¨é›†éƒ¨|æ³¨æ„äº‹é …", " ", text)
        return text[:max_chars]
    except Exception:
        return ""

# =========================
# Evidence åé›†
# =========================
TASKS = {
    "CVC": "CVCã‚’ç«‹ã¡ä¸Šã’ã¦ã„ã‚‹ã‹",
    "LP": "LPï¼ˆãƒ•ã‚¡ãƒ³ãƒ‰ã®ãƒªãƒŸãƒ†ãƒƒãƒ‰ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ï¼‰å‡ºè³‡ã‚’ã—ã¦ã„ã‚‹ã‹",
    "AI_Robotics": "AI/Roboticsã¨äº‹æ¥­ã‚·ãƒŠã‚¸ãƒ¼ãŒã‚ã‚‹ã‹",
    "Healthcare": "Healthcareã¨äº‹æ¥­ã‚·ãƒŠã‚¸ãƒ¼ãŒã‚ã‚‹ã‹",
    "Climate": "Climate techã¨äº‹æ¥­ã‚·ãƒŠã‚¸ãƒ¼ãŒã‚ã‚‹ã‹",
}

def _queries_for(company: str) -> Dict[str, List[str]]:
    quoted = f"\"{company.strip()}\""
    return {
        "CVC": [
            f"{quoted} CVC ã‚³ãƒ¼ãƒãƒ¬ãƒ¼ãƒˆãƒ™ãƒ³ãƒãƒ£ãƒ¼ã‚­ãƒ£ãƒ”ã‚¿ãƒ« ç«‹ã¡ä¸Šã’ æŠ•è³‡å­ä¼šç¤¾",
            f"{quoted} corporate venture capital CVC fund launch investing arm",
        ],
        "LP": [
            f"{quoted} LP å‡ºè³‡ ãƒªãƒŸãƒ†ãƒƒãƒ‰ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ ãƒ™ãƒ³ãƒãƒ£ãƒ¼ãƒ•ã‚¡ãƒ³ãƒ‰ å‡ºè³‡å‚ç”»",
            f"{quoted} limited partner LP commitment venture fund investor",
        ],
        "AI_Robotics": [
            f"{quoted} AI ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ ææº å‡ºè³‡ å…±åŒé–‹ç™º ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—",
            f"{quoted} AI robotics partnership investment startup collaboration",
        ],
        "Healthcare": [
            f"{quoted} ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢ åŒ»ç™‚ ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ˜ãƒ«ã‚¹ ææº å‡ºè³‡ å…±åŒç ”ç©¶",
            f"{quoted} healthcare medtech digital health partnership investment",
        ],
        "Climate": [
            f"{quoted} è„±ç‚­ç´  ã‚¯ãƒ©ã‚¤ãƒ¡ãƒ¼ãƒˆãƒ†ãƒƒã‚¯ å†ç”Ÿå¯èƒ½ã‚¨ãƒãƒ«ã‚®ãƒ¼ æ°´ç´  CCS ææº å‡ºè³‡",
            f"{quoted} climate tech decarbonization renewable hydrogen CCS partnership investment",
        ],
    }

@st.cache_data(show_spinner=False, ttl=60*60)
def gather_evidence(company: str, per_query_limit: int = 6, per_task_urls: int = 6) -> Dict[str, List[Dict[str, str]]]:
    queries = _queries_for(company)
    ev_raw: Dict[str, List[Dict[str, str]]] = {}
    for k, qlist in queries.items():
        bucket = []
        for q in qlist:
            time.sleep(0.2)
            bucket.extend(google_search(q, num=per_query_limit))
        seen = set(); uniq = []
        for it in bucket:
            u = it["link"]
            if u in seen: continue
            seen.add(u); uniq.append(it)
        ev_raw[k] = uniq[:per_task_urls]
    return ev_raw

@st.cache_data(show_spinner=False, ttl=60*60)
def hydrate_evidence_with_content(evidence: Dict[str, List[Dict[str, str]]], max_sources_per_task: int = 5) -> Dict[str, List[Dict[str, str]]]:
    out: Dict[str, List[Dict[str, str]]] = {}
    for task, items in evidence.items():
        enriched = []
        for it in items[:max_sources_per_task]:
            url = it["link"]
            body = fetch_text(url, max_chars=5000)
            enriched.append({"title": it.get("title",""), "link": url, "snippet": it.get("snippet",""), "body": body})
        out[task] = enriched
    return out

# =========================
# ä¼šç¤¾åãƒ•ã‚£ãƒƒãƒˆãƒ»ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
# =========================
JP_CORP_SUFFIXES = ["æ ªå¼ä¼šç¤¾", "ï¼ˆæ ªï¼‰", "(æ ª)", "ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹", "ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹æ ªå¼ä¼šç¤¾", "ã‚°ãƒ«ãƒ¼ãƒ—", "ã‚°ãƒ«ãƒ¼ãƒ—æ ªå¼ä¼šç¤¾"]
EN_CORP_SUFFIXES = ["Co., Ltd.", "Co.,Ltd.", "Company, Limited", "Inc.", "Incorporated", "Corporation", "Corp.", "Holdings", "Group", "Limited", "Ltd."]

def _normalize_name(n: str) -> str:
    s = unicodedata.normalize("NFKC", n or "").strip()
    s = re.sub(r"\s+", " ", s)
    return s

def _strip_corp_words(n: str) -> str:
    s = n
    for w in JP_CORP_SUFFIXES: s = s.replace(w, "")
    for w in EN_CORP_SUFFIXES: s = s.replace(w, "")
    s = s.replace("Kabushiki Kaisha", "").replace("K.K.", "")
    s = re.sub(r"[.,ãƒ»ï¼/|ï½œ\\\-â€-â€“â€”~ã€œ()\[\]{}ï¼œï¼<>]", " ", s)
    s = re.sub(r"\s+", "", s).lower()
    return s

def _variants_for_target(company: str) -> List[str]:
    base = _normalize_name(company)
    v = {base}
    if base.startswith("æ ªå¼ä¼šç¤¾"): v.add(base.replace("æ ªå¼ä¼šç¤¾", "", 1).strip())
    if base.endswith("æ ªå¼ä¼šç¤¾"):   v.add(base.replace("æ ªå¼ä¼šç¤¾", "").strip())
    return list({_strip_corp_words(x) for x in v})

COMPANY_PATTERNS = [
    r"æ ªå¼ä¼šç¤¾\s*([^\sã€ã€‚ï¼šã€Œã€ã€ã€()ï¼ˆï¼‰ã€ã€‘\n]{1,30})",
    r"([^\sã€ã€‚ï¼šã€Œã€ã€ã€()ï¼ˆï¼‰ã€ã€‘\n]{1,30})\s*æ ªå¼ä¼šç¤¾",
    r"ï¼ˆæ ªï¼‰\s*([^\sã€ã€‚ï¼šã€Œã€ã€ã€()ï¼ˆï¼‰ã€ã€‘\n]{1,30})",
    r"([A-Z][A-Za-z0-9&.\- ]{1,60})\s+(?:Co\.?,?\s*Ltd\.?|Inc\.|Corporation|Corp\.|Holdings|Group|Limited|Ltd\.)",
]

def _extract_company_like_names(text: str) -> List[str]:
    if not text: return []
    names = []
    for pat in COMPANY_PATTERNS:
        for m in re.findall(pat, text):
            if isinstance(m, tuple): m = m[0]
            nm = _normalize_name(m)
            if 1 <= len(nm) <= 60: names.append(nm)
    return names

def _company_fit_score_for_item(company: str, title: str, snippet: str, body: str) -> Tuple[float, Counter, int]:
    target_vars = _variants_for_target(company)
    title_n = unicodedata.normalize("NFKC", title or "")
    snip_n  = unicodedata.normalize("NFKC", snippet or "")
    body_n  = unicodedata.normalize("NFKC", body or "")

    def count_target(s: str) -> int:
        c = 0
        s_norm = _strip_corp_words(s)
        for tv in target_vars:
            if not tv: continue
            c += len(re.findall(re.escape(tv), s_norm, flags=re.IGNORECASE))
        return c

    title_hit = count_target(title_n) > 0
    snip_hit  = count_target(snip_n)  > 0
    body_cnt  = count_target(body_n)

    names = _extract_company_like_names(body_n + " " + title_n)
    other_counter = Counter()
    for n in names:
        norm = _strip_corp_words(n)
        if norm and norm not in target_vars:
            other_counter[norm] += 1
    max_other = max(other_counter.values()) if other_counter else 0

    score = (2 if title_hit else 0) + (1 if snip_hit else 0) + body_cnt - 2 * max_other
    return score, other_counter, body_cnt

def filter_evidence_by_company(company: str, evidence_enriched: Dict[str, List[Dict[str, str]]]) -> Dict[str, List[Dict[str, str]]]:
    out: Dict[str, List[Dict[str, str]]] = {}
    for task, items in evidence_enriched.items():
        scored = []
        for it in items:
            s, others, tgt_body_cnt = _company_fit_score_for_item(company, it.get("title",""), it.get("snippet",""), it.get("body",""))
            scored.append((s, tgt_body_cnt, it, others))
        scored.sort(key=lambda x: x[0], reverse=True)
        kept = []
        for s, tgt_cnt, it, _ in scored:
            if tgt_cnt >= 1 and s >= 1.0:
                kept.append(it)
        out[task] = kept
    return out

# =========================
# OpenAI Reasoning
# =========================
PROMPT_SYSTEM = (
    "You are an analyst of corporateâ€“startup collaboration. "
    "Return STRICT JSON only with the exact schema requested. "
    "Be conservative: choose 'Unclear' unless there is direct evidence."
)

PROMPT_USER_TEMPLATE = """
You will judge one company across tasks with provided web evidence (titles, snippets, and fetched page body).
Company: {company}

Decision hygiene (company-specific):
- Ignore any evidence where the target company name does NOT appear in the title or body at least once.
- If multiple specific company names appear, treat the article as valid ONLY if the target's mentions are not fewer than the most frequently mentioned other company name in that article. Otherwise, mark as 'Unclear'.

Definitions and decision rules (apply strictly):
- CVC: The company has its own corporate venture capital arm or investment subsidiary (e.g., 'CVC', 'corporate venture capital', 'investment subsidiary', 'capital partners'). One-off venture investments without a dedicated arm â†’ do NOT mark 'Yes'.
- LP: The company has committed capital as a limited partner to an external venture fund (e.g., 'LP', 'limited partner', 'commitment', 'å‡ºè³‡', 'ãƒªãƒŸãƒ†ãƒƒãƒ‰ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼'). If only investing directly as a CVC without LP commitment, mark 'No' (unless LP is also evidenced).
- Synergy (AI_Robotics / Healthcare / Climate): Mark 'Yes' only if there is concrete evidence of products, partnerships, investments, pilots, or stated strategic focus that clearly connects to the domain. Generic news unrelated to the company's business â†’ 'Unclear'.

Hard constraints:
- Output must be valid JSON (UTF-8, no trailing commas, no comments).
- For each task, set 'label' âˆˆ {{'Yes','No','Unclear'}}, 'confidence' âˆˆ [0,1].
- 'reason_ja': â‰¤100 Japanese characters. 'reason_en': 1â€“2 sentences English.
- 'evidence_urls': include up to 3 URLs, but ONLY from the provided evidence links. Do NOT invent URLs.
- If signals conflict or are outdated without follow-ups, prefer 'Unclear'.
- If CVC is 'Yes' and LP also 'Yes', ensure reasons clearly distinguish between the two.
- If no relevant evidence, use 'Unclear' with confidence 0.2.

Return JSON in the exact schema:
{{
  "per_task": {{
    "CVC":         {{"label":"","confidence":0.0,"reason_ja":"","reason_en":"","evidence_urls":[]}},
    "LP":          {{"label":"","confidence":0.0,"reason_ja":"","reason_en":"","evidence_urls":[]}},
    "AI_Robotics": {{"label":"","confidence":0.0,"reason_ja":"","reason_en":"","evidence_urls":[]}},
    "Healthcare":  {{"label":"","confidence":0.0,"reason_ja":"","reason_en":"","evidence_urls":[]}},
    "Climate":     {{"label":"","confidence":0.0,"reason_ja":"","reason_en":"","evidence_urls":[]}}
  }},
  "x_post": {{"jp":"","en":""}}
}}

Evidence (grouped by task). Each item has fields: title, link, snippet, body (first kilobytes of fetched page).
{evidence_json}
"""

def _safe_json_loads(text: str) -> dict:
    try:
        return json.loads(text)
    except Exception:
        m = re.search(r"\{.*\}\s*$", text, re.S)
        if m:
            try:
                return json.loads(m.group(0))
            except Exception:
                pass
        return {}

def ask_openai_reasoning(company: str, evidence_enriched: Dict[str, List[Dict[str, str]]]) -> Dict[str, Any]:
    if not _oai:
        return {}
    prompt_user = PROMPT_USER_TEMPLATE.format(
        company=company,
        evidence_json=json.dumps(evidence_enriched, ensure_ascii=False)[:120000]
    )
    resp = _oai.responses.create(
        model=MODEL_REASON,
        temperature=0.0,
        max_output_tokens=1500,
        input=f"System:\n{PROMPT_SYSTEM}\n\nUser:\n{prompt_user}",
    )
    text = resp.output_text
    data = _safe_json_loads(text)
    if not data or "per_task" not in data:
        resp2 = _oai.responses.create(
            model=MODEL_REASON,
            temperature=0.0,
            max_output_tokens=1500,
            input=("System:\n" + PROMPT_SYSTEM + "\n\nUser:\nReturn ONLY valid JSON per the schema. "
                   "If previous attempt failed, correct and resend the JSON.\n" + prompt_user),
        )
        text = resp2.output_text
        data = _safe_json_loads(text)

    skeleton = {
        "per_task": {k: {"label":"Unclear","confidence":0.2,"reason_ja":"","reason_en":"","evidence_urls":[]} for k in TASKS},
        "x_post": {"jp":"","en":""}
    }
    try:
        merged = skeleton
        if isinstance(data, dict):
            merged["x_post"].update(data.get("x_post", {}))
            pt = data.get("per_task", {})
            for k in TASKS:
                if isinstance(pt.get(k), dict):
                    merged["per_task"][k].update({
                        kk: pt[k].get(kk, merged["per_task"][k][kk])
                        for kk in ["label","confidence","reason_ja","reason_en","evidence_urls"]
                    })
        return merged
    except Exception:
        return skeleton

# =========================
# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ”¯æ´ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
def _to_b64_csv(df: pd.DataFrame) -> str:
    csv = df.to_csv(index=False)
    return base64.b64encode(csv.encode("utf-8")).decode()

def _auto_download(b64: str, filename: str):
    st.components.v1.html(
        f"""
        <html><body>
        <a id="autodl" href="data:text/csv;base64,{b64}" download="{filename}"></a>
        <script>document.getElementById('autodl').click();</script>
        </body></html>
        """,
        height=0,
    )

# =========================
# Streamlit UIï¼ˆSecrets ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãªã—ï¼‰
# =========================
st.set_page_config(page_title="Corporateâ€“Startup Fit Checker+", layout="wide")
st.title("ğŸ¢â¡ï¸ğŸ¤ğŸš€ Corporateâ€“Startup Fit Checker+")
st.caption("Cåˆ—=ä¼šç¤¾åã€‚è¨¼è·¡â†’æœ¬æ–‡å–å¾—â†’ä¼šç¤¾åã‚¹ã‚³ã‚¢ã§ä»–ç¤¾è¨˜äº‹ã‚’é™¤å¤–â†’OpenAIã§åˆ¤å®šã€‚ä¸­é–“CSVã‚’è‡ªå‹•ä¿å­˜ã€‚")

cols = st.columns(4)
with cols[0]:
    uploaded = st.file_uploader("Excel ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆCåˆ—=ä¼šç¤¾åï¼‰", type=["xlsx", "xls"])
with cols[1]:
    limit = st.number_input("å‡¦ç†ä»¶æ•°ã®ä¸Šé™", 1, 20000, 200, 50)
with cols[2]:
    max_sources = st.slider("å„ã‚¿ã‚¹ã‚¯ã®æœ€å¤§å‚ç…§URLæ•°", 1, 8, 5)
with cols[3]:
    checkpoint_every = st.number_input("è‡ªå‹•ä¿å­˜ï¼ˆç¤¾ã”ã¨ï¼‰", 1, 200, 25, 5)

# â–¼ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®æ°¸ç¶šåŒ–ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ï¼‰
if uploaded is not None:
    st.session_state["uploaded_bytes"] = uploaded.getvalue()
    st.session_state["uploaded_name"] = getattr(uploaded, "name", "input.xlsx")

# è‡ªå‹•DLã®ä¸€å›åˆ¶å¾¡
if "auto_dl_done" not in st.session_state:
    st.session_state.auto_dl_done = False

run = st.button("è§£æã‚¹ã‚¿ãƒ¼ãƒˆ", type="primary", disabled=("uploaded_bytes" not in st.session_state))

if run and ("uploaded_bytes" in st.session_state):
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰èª­ã¿ç›´ã—ï¼ˆé€”ä¸­å†å®Ÿè¡Œã§ã‚‚ç¶™ç¶šå¯èƒ½ï¼‰
    data_bytes = st.session_state["uploaded_bytes"]
    filelike = io.BytesIO(data_bytes)

    df = pd.read_excel(filelike)
    if df.shape[1] >= 3:
        companies = df.iloc[:, 2].dropna().astype(str).tolist()
    else:
        companies = df.iloc[:, -1].dropna().astype(str).tolist()
    companies = companies[:int(limit)]

    rows = []
    progress = st.progress(0.0)
    status = st.empty()
    tabs = st.tabs(["é€²æ—", "æœ€çµ‚ãƒ†ãƒ¼ãƒ–ãƒ« / ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", "è©³ç´°ãƒ­ã‚°"])
    with tabs[0]:
        st.write("æ¤œç´¢â†’æœ¬æ–‡å–å¾—â†’ä¼šç¤¾åãƒ•ã‚£ãƒ«ã‚¿â†’åˆ¤å®šã‚’ãƒ«ãƒ¼ãƒ—ã€‚ä¸€å®šç¤¾æ•°ã”ã¨ã«è‡ªå‹•ã§CSVä¿å­˜ã—ã¾ã™ã€‚")

    detail_log = []
    st.session_state.auto_dl_done = False  # æ–°è¦é–‹å§‹ã”ã¨ã«ãƒªã‚»ãƒƒãƒˆ

    for i, company in enumerate(companies, 1):
        status.info(f"Searching & analyzing: {company}")
        try:
            ev = gather_evidence(company)
            ev_enriched = hydrate_evidence_with_content(ev, max_sources_per_task=max_sources)
            ev_enriched = filter_evidence_by_company(company, ev_enriched)
            reasoning = ask_openai_reasoning(company, ev_enriched) if OPENAI_API_KEY else {"per_task": {}, "x_post": {"jp":"", "en":""}}
            per_task = reasoning.get("per_task", {})
            x_post = reasoning.get("x_post", {"jp":"", "en":""})

            def cell(task: str, field: str, default=""):
                return per_task.get(task, {}).get(field, default)

            row = {
                "company": company,
                "CVC":        cell("CVC", "label", "Unclear"),
                "LP":         cell("LP", "label", "Unclear"),
                "AI_Robotics":cell("AI_Robotics", "label", "Unclear"),
                "Healthcare": cell("Healthcare", "label", "Unclear"),
                "Climate":    cell("Climate", "label", "Unclear"),
                "CVC_conf":        cell("CVC", "confidence", ""),
                "LP_conf":         cell("LP", "confidence", ""),
                "AI_Robotics_conf":cell("AI_Robotics", "confidence", ""),
                "Healthcare_conf": cell("Healthcare", "confidence", ""),
                "Climate_conf":    cell("Climate", "confidence", ""),
                "CVC_reason_ja":        cell("CVC", "reason_ja", ""),
                "LP_reason_ja":         cell("LP", "reason_ja", ""),
                "AI_Robotics_reason_ja":cell("AI_Robotics", "reason_ja", ""),
                "Healthcare_reason_ja": cell("Healthcare", "reason_ja", ""),
                "Climate_reason_ja":    cell("Climate", "reason_ja", ""),
                "CVC_reason_en":        cell("CVC", "reason_en", ""),
                "LP_reason_en":         cell("LP", "reason_en", ""),
                "AI_Robotics_reason_en":cell("AI_Robotics", "reason_en", ""),
                "Healthcare_reason_en": cell("Healthcare", "reason_en", ""),
                "Climate_reason_en":    cell("Climate", "reason_en", ""),
                "CVC_urls":        "; ".join(per_task.get("CVC", {}).get("evidence_urls", [])),
                "LP_urls":         "; ".join(per_task.get("LP", {}).get("evidence_urls", [])),
                "AI_Robotics_urls":"; ".join(per_task.get("AI_Robotics", {}).get("evidence_urls", [])),
                "Healthcare_urls": "; ".join(per_task.get("Healthcare", {}).get("evidence_urls", [])),
                "Climate_urls":    "; ".join(per_task.get("Climate", {}).get("evidence_urls", [])),
                "x_post_jp": x_post.get("jp", ""),
                "x_post_en": x_post.get("en", "")
            }
            rows.append(row)

            detail_log.append({"company": company, "evidence": ev_enriched, "result": reasoning})
        except Exception as e:
            rows.append({"company": company, "error": str(e)})
            detail_log.append({"company": company, "error": str(e), "trace": traceback.format_exc()})

        # é€²æ—æ›´æ–°ï¼ˆã‚µãƒ¼ãƒãƒ»ãƒ–ãƒ©ã‚¦ã‚¶åŒæ–¹ã®ã‚¢ã‚¤ãƒ‰ãƒ«åˆ‡æ–­å›é¿ã«æœ‰åŠ¹ï¼‰
        progress.progress(i/len(companies))
        time.sleep(0.02)

        # â–¼ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ»è‡ªå‹•DLï¼ˆä¸€å®šç¤¾æ•°ã”ã¨ï¼‰
        if i % int(checkpoint_every) == 0:
            partial_df = pd.DataFrame(rows)
            b64 = _to_b64_csv(partial_df)
            _auto_download(b64, f"corporate_fit_checkpoint_{i:05d}.csv")
            with tabs[1]:
                st.toast(f"ä¸­é–“CSVï¼ˆ{i}ç¤¾æ™‚ç‚¹ï¼‰ã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚", icon="âœ…")
                st.dataframe(partial_df.tail(20), use_container_width=True)

    # ===== æœ€çµ‚çµæœ =====
    out = pd.DataFrame(rows)
    with tabs[1]:
        st.success("è§£æå®Œäº†ï¼")
        st.dataframe(out, use_container_width=True)

        csv_b64 = _to_b64_csv(out)
        # æ‰‹å‹•DLï¼ˆä¿é™ºï¼‰
        st.download_button(
            "æœ€çµ‚CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=base64.b64decode(csv_b64),
            file_name="corporate_fit_with_reasons.csv",
            mime="text/csv",
        )
        # è‡ªå‹•DLï¼ˆæœ€çµ‚ï¼‰
        _auto_download(csv_b64, "corporate_fit_with_reasons.csv")
        st.info("æœ€çµ‚CSVã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸå ´åˆã¯ãƒœã‚¿ãƒ³ã‹ã‚‰ä¿å­˜ã—ã¦ãã ã•ã„ã€‚")

    with tabs[2]:
        for block in detail_log:
            with st.expander(f"ğŸ” {block.get('company')} ã®è©³ç´°"):
                if "error" in block:
                    st.error(block["error"])
                    st.code(block.get("trace",""))
                else:
                    st.markdown("**å‚ç…§ Evidenceï¼ˆå„ã‚¿ã‚¹ã‚¯ï¼‰**")
                    for task, items in block["evidence"].items():
                        st.markdown(f"- **{task}**")
                        for it in items:
                            st.write(f"[{it['title']}]({it['link']})")
                            if it.get("snippet"):
                                st.caption(it["snippet"])
                    st.markdown("**LLM å‡ºåŠ›ï¼ˆJSONï¼‰**")
                    st.code(json.dumps(block["result"], ensure_ascii=False, indent=2))
