import re
import textwrap
import streamlit as st
import pandas as pd
from urllib.parse import urlparse
from openai import OpenAI

st.set_page_config(page_title="X Post Drafts (Scholar/Google Alerts)", layout="centered")
st.title("ğŸ§ª Xãƒã‚¹ãƒˆä¸‹æ›¸ãã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ï¼ˆScholar/Google Alertså¯¾å¿œï¼‰")

st.write("è²¼ã‚Šä»˜ã‘ãŸã‚¢ãƒ©ãƒ¼ãƒˆæœ¬æ–‡ã‹ã‚‰ **æ—¥æœ¬èª(ãƒ†ã‚­ã‚¹ãƒˆã®ã¿)** / **è‹±èª(ãƒªãƒ³ã‚¯ä»˜ã)** ã®XæŠ•ç¨¿æ¡ˆã‚’ã€ç„¡æ–™ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä¸Šé™å†…ã§ç”Ÿæˆã—ã¾ã™ã€‚")

client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# --- Xã®ç„¡æ–™æŠ•ç¨¿åˆ¶é™ã¨URLæ›ç®— ---
X_LIMIT = 280
TCO_URL_LEN = 23  # t.coçŸ­ç¸®æ›ç®—ï¼ˆç„¡æ–™ã§ã‚‚åŒã˜æ‰±ã„ã‚’æƒ³å®šï¼‰

def extract_urls(text: str) -> list[str]:
    url_re = re.compile(r'https?://\S+')
    return url_re.findall(text)

def sanitize_url(u: str) -> str:
    # ä½™è¨ˆãªãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã¯ç°¡æ˜“ã«é™¤å»
    try:
        p = urlparse(u)
        if not p.scheme:
            return u
        base = f"{p.scheme}://{p.netloc}{p.path}"
        return base
    except Exception:
        return u

def x_count_len(text: str) -> int:
    """URLã‚’23æ–‡å­—æ›ç®—ã§åˆè¨ˆæ–‡å­—æ•°ã‚’æ¦‚ç®—ã€‚"""
    urls = extract_urls(text)
    tmp = text
    for u in urls:
        tmp = tmp.replace(u, " " * TCO_URL_LEN, 1)  # ç½®æ›ã§é•·ã•ã ã‘åˆã‚ã›ã‚‹
    return len(tmp)

def clip_to_limit(text: str, limit: int = X_LIMIT) -> str:
    if x_count_len(text) <= limit:
        return text
    # æœ«å°¾ã«â€¦ã‚’ã¤ã‘ã‚‹ä½™åœ°ã‚’ç¢ºä¿
    ell = "â€¦"
    # URLã¯å£Šã•ãªã„ï¼šä¸€æ—¦URLã‚’é€€é¿ã—ã€æœ¬æ–‡ã ã‘ã‚’ãƒˆãƒªãƒ ã—ã¦ã‹ã‚‰æˆ»ã™
    urls = extract_urls(text)
    base = text
    for u in urls:
        base = base.replace(u, "")  # å…ˆã«æœ¬æ–‡ã ã‘ã«
    base = base.strip()
    # ã§ãã‚‹ã ã‘æ–‡ã‚’å£Šã•ãšã«çŸ­ç¸®
    while x_count_len(base + ((" " + " ".join(urls)) if urls else "")) + len(ell) > limit and len(base) > 0:
        base = base[:-1]
    clipped = base.rstrip() + ell
    final = clipped + ((" " + " ".join(urls)) if urls else "")
    # å¿µã®ãŸã‚æœ€çµ‚ãƒã‚§ãƒƒã‚¯
    if x_count_len(final) > limit:
        # ã•ã‚‰ã«å‰Šã‚‹ï¼ˆå®‰å…¨å´ï¼‰
        over = x_count_len(final) - limit
        clipped2 = clipped[:-max(1, over)]
        final = clipped2
    return final.strip()

def summarize_alert(alert_text: str) -> dict:
    """ã‚¿ã‚¤ãƒˆãƒ«/è¦ç‚¹/è‹±èªã‚¿ã‚¤ãƒˆãƒ«ãªã©ã‚’æŠ½å‡º"""
    sys = "You are a helpful research assistant for social posts."
    prompt = f"""
From the following Google Scholar/Google Alert text, extract:
1) short Japanese title (<=60 chars),
2) short English title (<=80 chars),
3) 1-2 sentence Japanese summary (<=180 chars),
4) 1 sentence English summary (<=220 chars).
Return strict JSON with keys: ja_title, en_title, ja_sum, en_sum.

Alert:
"""
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role":"system","content":sys},
                  {"role":"user","content":prompt}],
        temperature=0.2,
    )
    import json, re
    txt = resp.choices[0].message.content.strip()
    m = re.search(r"\{.*\}", txt, re.S)
    data = {"ja_title":"", "en_title":"", "ja_sum":"", "en_sum":""}
    if m:
        try:
            data.update(json.loads(m.group(0)))
        except Exception:
            pass
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    for k in data:
        if not data[k]:
            data[k] = ""
    return data

def build_ja_post(meta: dict, hashtags: list[str]) -> str:
    body = f"{meta['ja_title']} â€” {meta['ja_sum']}".strip(" â€”")
    if hashtags:
        body += "\n" + " ".join(hashtags)
    return clip_to_limit(body)

def build_en_post(meta: dict, urls: list[str], hashtags: list[str]) -> str:
    # å‡ºå…¸URLã¯1æœ¬ã ã‘ï¼ˆå„ªå…ˆï¼šScholaræœ¬æ–‡/è«–æ–‡URLï¼‰
    link = urls[0] if urls else ""
    link = sanitize_url(link) if link else ""
    pieces = [f"{meta['en_title']}".strip(), meta['en_sum'].strip(), link.strip()]
    body = " â€” ".join([p for p in pieces if p])
    if hashtags:
        body += "\n" + " ".join(hashtags)
    return clip_to_limit(body)

st.markdown("**å…¥åŠ›**ï¼ˆã‚¢ãƒ©ãƒ¼ãƒˆæœ¬æ–‡ã‚’è²¼ã‚Šä»˜ã‘ï¼URLã¯è‡ªå‹•æŠ½å‡ºï¼‰")
alert = st.text_area("Google Scholar Alert / Google Alert ã®æœ¬æ–‡", height=220, placeholder="ã‚¢ãƒ©ãƒ¼ãƒˆæœ¬æ–‡ã‚„è¦‹å‡ºã—ï¼‹URLã‚’è²¼ã‚Šä»˜ã‘")

col1, col2 = st.columns(2)
with col1:
    ja_tags = st.text_input("æ—¥æœ¬èªãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ï¼ˆä»»æ„ã€ç©ºç™½åŒºåˆ‡ã‚Šï¼‰", value="#ç ”ç©¶ #æ”¿ç­– #VC")
with col2:
    en_tags = st.text_input("English hashtags (optional, space-separated)", value="#research #policy #VC")

if st.button("ãƒ‰ãƒ©ãƒ•ãƒˆç”Ÿæˆ"):
    if not alert.strip():
        st.warning("ã¾ãšã‚¢ãƒ©ãƒ¼ãƒˆæœ¬æ–‡ã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("æŠ½å‡ºãƒ»è¦ç´„ä¸­..."):
            urls = extract_urls(alert)
            meta = summarize_alert(alert)

        ja_hashtags = [t for t in ja_tags.split() if t.startswith("#")]
        en_hashtags = [t for t in en_tags.split() if t.startswith("#")]

        ja_post = build_ja_post(meta, ja_hashtags)
        en_post = build_en_post(meta, urls, en_hashtags)

        st.subheader("ğŸ“Œ ç”Ÿæˆçµæœï¼ˆXç„¡æ–™æ  280æ–‡å­—å†…ï¼‰")
        st.markdown("**æ—¥æœ¬èªãƒ‰ãƒ©ãƒ•ãƒˆ**")
        st.code(ja_post, language="markdown")
        st.caption(f"é•·ã•: {len(ja_post)}ï¼ˆURLã¯å«ã¾ã‚Œã¦ã„ãªã„æƒ³å®šï¼‰ / ä¸Šé™ {X_LIMIT}")

        st.markdown("**English draft (w/ source link)**")
        st.code(en_post, language="markdown")
        st.caption(f"æ¦‚ç®—é•·ã•(xæ›ç®—): {len(en_post)}ï¼ˆURLã¯23æ–‡å­—æ›ç®—ï¼‰ / ä¸Šé™ {X_LIMIT}")

        # CSVå‡ºåŠ›ï¼ˆBOMä»˜ãUTF-8ã§Excelå¯¾ç­–ï¼‰
        df = pd.DataFrame([{
            "ja_title": meta.get("ja_title",""),
            "ja_post": ja_post,
            "en_title": meta.get("en_title",""),
            "en_post": en_post,
            "source_url": urls[0] if urls else "",
        }])
        csv = df.to_csv(index=False).encode("utf-8-sig")
        st.download_button("â¬‡ï¸ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆUTF-8/BOMï¼‰", data=csv, file_name="x_post_drafts.csv", mime="text/csv")

st.markdown("---")
st.caption("â€» æ–‡å­—æ•°æ›ç®—ã¯Xç„¡æ–™ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®280æ–‡å­—ä¸Šé™ãŠã‚ˆã³URL=23æ–‡å­—æƒ³å®šã«æº–æ‹ ï¼ˆå‚è€ƒ: character limit 280 / URLã¯t.coã§23æ–‡å­—æ‰±ã„ï¼‰ã€‚")
