# x_news_draft_app_bs.py
# ------------------------------------------------------------
# Google Alerts ã® HTML ã‚’è²¼ä»˜/ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â†’ BeautifulSoup ã§è¨˜äº‹æŠ½å‡º
# â†’ LLM ã§ã€Œæœ€ã‚‚è‰¯ã•ãã†ãªè¨˜äº‹ã€ã‚’é¸å®š â†’ X æŠ•ç¨¿å‘ã‘ãƒ‰ãƒ©ãƒ•ãƒˆï¼ˆæ—¥/è‹±ï¼‰ã‚’ç”Ÿæˆ
# ç’°å¢ƒå¤‰æ•°: OPENAI_API_KEYï¼ˆStreamlit Cloud ã§ã¯ Secrets ã§è¨­å®šï¼‰
# ------------------------------------------------------------

import os
import re
import json
import time
from typing import List, Dict, Any, Optional

import streamlit as st
from bs4 import BeautifulSoup

# OpenAI v1 SDK
try:
    from openai import OpenAI
except Exception:
    st.error("openai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚requirements.txt ã« 'openai>=1.40.0' ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
    raise

# -----------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -----------------------------
def extract_real_url(href: str) -> str:
    """Googleã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‹ã‚‰å®ŸURLã‚’å–ã‚Šå‡ºã™"""
    if not href:
        return ""
    m = re.search(r"[?&]url=(https?://[^&]+)", href)
    if m:
        return m.group(1)
    return href

def clean_whitespace(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def parse_google_alert_html(html: str) -> List[Dict[str, str]]:
    """
    Google Alerts ãƒ¡ãƒ¼ãƒ«HTMLã‹ã‚‰è¨˜äº‹å€™è£œã‚’æŠ½å‡º
    è¿”ã‚Šå€¤: [{title, url, source, snippet}]
    """
    soup = BeautifulSoup(html, "html.parser")

    # å„ <a> ã‚’èµ°æŸ»ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚‰ã—ã„ã‚‚ã®ã ã‘æŠ½å‡º
    candidates = []
    for a in soup.find_all("a", href=True):
        text = clean_whitespace(a.get_text(" ", strip=True))
        href = a["href"]

        # é™¤å¤–æ¡ä»¶ï¼šã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†ç³»ã€å…±æœ‰ãƒœã‚¿ãƒ³ç­‰
        if (
            not text
            or "google.com/alerts" in href
            or "alerts/share" in href
            or "facebook" in href.lower()
            or "twitter" in href.lower()
            or "Flag as irrelevant" in text
            or text.lower() in {"facebook", "twitter"}
        ):
            continue

        url = extract_real_url(href)

        # URL ãŒåª’ä½“è¨˜äº‹ã£ã½ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé›‘ã«åˆ¤å®šï¼‰
        if not url.startswith("http"):
            continue
        if "google.com/url" in url:
            # url= ãŒç„¡ã„ç¨€ãªã‚±ãƒ¼ã‚¹
            continue

        # å›²ã£ã¦ã„ã‚‹ã‚»ãƒ«ãªã©ã‹ã‚‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆãƒ»åª’ä½“åã‚’æ¨æ¸¬
        td = a.find_parent("td")
        block_text = clean_whitespace(td.get_text(" ", strip=True)) if td else ""
        # åª’ä½“åã‚’ " source " ã¨ã—ã¦æ‹¾ã„ã‚„ã™ã„å ´æ‰€ã‹ã‚‰æ¨å®šï¼ˆç°¡æ˜“ï¼‰
        # ä¾‹: "Chronicle of Philanthropy ... Venture-Capital-Backed ..."
        source = ""
        # <div>ã§åª’ä½“åãŒåˆ¥ã‚¿ã‚°ã«ã‚ã‚‹ã“ã¨ãŒå¤šã„ã®ã§å…„å¼Ÿè¦ç´ ã‚‚è¦‹ã‚‹
        if td:
            source_div = td.find("div", style=re.compile("font-size:12px", re.I))
            if source_div:
                s_txt = clean_whitespace(source_div.get_text(" ", strip=True))
                # åª’ä½“åã‚‰ã—ãçŸ­ã„éƒ¨åˆ†ã‚’æŠ½å‡º
                if s_txt and len(s_txt) <= 60:
                    source = s_txt

        snippet = block_text
        if len(snippet) > 240:
            snippet = snippet[:240] + "â€¦"

        candidates.append(
            {
                "title": text,
                "url": url,
                "source": source,
                "snippet": snippet,
            }
        )

    # URLã§é‡è¤‡æ’é™¤
    unique = {}
    for c in candidates:
        unique[c["url"]] = c
    results = list(unique.values())

    # ã‚¿ã‚¤ãƒˆãƒ«ç­‰ãŒæ¥µç«¯ã«çŸ­ã„/ãƒã‚¤ã‚ºãªã‚‚ã®ã‚’å‰Šã‚‹
    results = [r for r in results if len(r["title"]) >= 8]

    return results

def safe_trim(text: str, max_len: int) -> str:
    """ã‚³ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆé•·ã§280ä»¥å†…ã«åã‚ã‚‹ç°¡æ˜“ãƒˆãƒªãƒ """
    text = text.strip()
    return text if len(text) <= max_len else text[: max_len - 1].rstrip() + "â€¦"

def build_ja_post(title: str, source: str, key_points: str, hashtag_hint: Optional[str] = None) -> str:
    """
    æ—¥æœ¬èªãƒã‚¹ãƒˆ: æ–‡ç« ã®ã¿ï¼ˆãƒªãƒ³ã‚¯ç„¡ã—ï¼‰ã€280æ–‡å­—ä»¥å†…
    """
    parts = []
    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç°¡æ˜“ã«å’Œè¨³æ¸ˆã¿æƒ³å®šã®è¦ç´„ï¼ˆLLMå´ã§ä»˜ä¸ï¼‰ã«é ¼ã‚‹ãŸã‚ã€ã“ã“ã¯ key_points ã‚’ä¸­å¿ƒã«
    if key_points:
        parts.append(key_points.strip())
    if source:
        parts.append(f"ï¼ˆå‡ºæ‰€: {source}ï¼‰")
    if hashtag_hint:
        parts.append(hashtag_hint)
    text = " ".join([p for p in parts if p])
    return safe_trim(text, 280)

def build_en_post(title: str, url: str, source: str, key_points: str, hashtag_hint: Optional[str] = None) -> str:
    """
    è‹±èªãƒã‚¹ãƒˆ: æœ¬æ–‡+ãƒªãƒ³ã‚¯ï¼ˆURLä»˜ãï¼‰ã€280æ–‡å­—ä»¥å†…
    Xã¯URLã‚’t.coã«çŸ­ç¸®ã—ã¦ã‚‚ã‚«ã‚¦ãƒ³ãƒˆä¸Šã¯ã»ã¼å›ºå®š(ç´„23)ã ãŒã€å®Ÿè£…ç°¡æ˜“ã®ãŸã‚å…¨ä½“280ã§ãƒˆãƒªãƒ 
    """
    base = f"{key_points.strip()}" if key_points else title
    if source:
        base = f"{base} (via {source})"
    if hashtag_hint:
        base = f"{base} {hashtag_hint}"

    # URLã‚’æœ€å¾Œã«ã¤ã‘ã‚‹ã€‚å¿…è¦ãªã‚‰æœ¬æ–‡ãƒˆãƒªãƒ 
    # ä½™è£•ã‚’å°‘ã—ã¿ã¦ URL åˆ†ã®ã‚¹ãƒšãƒ¼ã‚¹ã‚‚è€ƒæ…®
    max_body = 280 - (len(url) + 1)  # 1ã¯ã‚¹ãƒšãƒ¼ã‚¹
    body = safe_trim(base, max_body)
    return f"{body} {url}"

# -----------------------------
# OpenAI å‘¼ã³å‡ºã—
# -----------------------------
def get_client() -> OpenAI:
    api_key = os.getenv("OPENAI_API_KEY", "")
    if not api_key:
        st.warning("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Cloud ã® Secrets ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    return OpenAI()

RATER_SYSTEM_PROMPT = """You are an expert editor for policy and venture capital news. 
Given a list of candidate articles (title, source, snippet, url), pick the SINGLE best one for an X post that would interest a PhD student focused on:
- Entrepreneurship and innovation
- Venture capital and entrepreneurial finance
- Public policy and institutional design
- Applied econometrics
- Cross-border investment

Scoring criteria (0-5 each):
- Relevance to those topics
- Novelty/timeliness (based on text)
- Credibility of source (if known)
- Policy/VC insight density

Return strict JSON with:
{
  "picked_index": <int zero-based>,
  "reason": "<short>",
  "key_points_en": "<1-2 sentence crisp English summary>",
  "key_points_ja": "<1-2 sentence crisp Japanese summary>",
  "hashtags_en": "<up to 3 short hashtags like #VentureCapital #Policy>",
  "hashtags_ja": "<å…¨è§’ãªã—ã®çŸ­ã„ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’æœ€å¤§3ã¤>"
}
"""

RATER_USER_TEMPLATE = """Candidates:
{items}

Please pick one. Respond JSON only.
"""

def rate_and_pick_article(client: OpenAI, items: List[Dict[str, str]], model: str = "gpt-4o-mini") -> Dict[str, Any]:
    # items ã‚’ç•ªå·ä»˜ãã§æ–‡å­—åˆ—åŒ–
    lines = []
    for i, a in enumerate(items):
        lines.append(
            f"[{i}] title: {a['title']}\nsource: {a.get('source','')}\nurl: {a['url']}\nsnippet: {a.get('snippet','')}\n"
        )
    prompt = RATER_USER_TEMPLATE.format(items="\n".join(lines))

    resp = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": RATER_SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
    )
    content = resp.choices[0].message.content.strip()
    # JSONã ã‘è¿”ã™å‰æ
    try:
        data = json.loads(content)
        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        idx = int(data.get("picked_index", 0))
        if idx < 0 or idx >= len(items):
            idx = 0
            data["picked_index"] = 0
        return data
    except Exception:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæœ€åˆã‚’é¸ã¶
        return {
            "picked_index": 0,
            "reason": "Fallback: JSON parse failed",
            "key_points_en": items[0]["title"],
            "key_points_ja": items[0]["title"],
            "hashtags_en": "#VentureCapital",
            "hashtags_ja": "#VC",
        }

# -----------------------------
# Streamlit UI
# -----------------------------
st.set_page_config(page_title="X News Draft (BeautifulSoup)", page_icon="ğŸ“°", layout="wide")
st.title("ğŸ“° Xãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‰ãƒ©ãƒ•ãƒˆï¼ˆGoogle Alerts HTMLå¯¾å¿œï¼‰")

with st.sidebar:
    st.header("è¨­å®š")
    model = st.selectbox("OpenAIãƒ¢ãƒ‡ãƒ«", ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini"], index=0)
    add_hashtags = st.checkbox("ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’ä»˜ä¸ã™ã‚‹", value=True)
    st.markdown("---")
    st.markdown("**ä½¿ã„æ–¹**")
    st.markdown("1) Google Alerts ã®ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡HTMLã‚’è²¼ã‚Šä»˜ã‘ã€ã¾ãŸã¯ `.html` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.markdown("2) æŠ½å‡ºçµæœã‚’ç¢ºèª â†’ ã€ãƒ‰ãƒ©ãƒ•ãƒˆç”Ÿæˆã€")
    st.markdown("3) ç”Ÿæˆã•ã‚ŒãŸæ—¥è‹±ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ã‚³ãƒ”ãƒ¼ï¼ˆè‹±èªã¯ãƒªãƒ³ã‚¯ä»˜ãï¼‰")

tab_input, tab_preview, tab_output = st.tabs(["â‘  å…¥åŠ›", "â‘¡ æŠ½å‡ºãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", "â‘¢ å‡ºåŠ›"])

with tab_input:
    st.subheader("HTML ã‚’è²¼ã‚Šä»˜ã‘")
    html_text = st.text_area(
        "ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ï¼ˆHTMLï¼‰ã‚’ãã®ã¾ã¾è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„",
        height=240,
        placeholder="ã“ã“ã« <div>... ã®ã‚ˆã†ãªHTMLã‚’è²¼ã‚Šä»˜ã‘",
    )
    st.write("ã¾ãŸã¯")
    uploaded = st.file_uploader("HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["html", "htm"])

    html_source = ""
    if uploaded is not None:
        try:
            html_source = uploaded.read().decode("utf-8", errors="ignore")
            st.success("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼: {e}")
    elif html_text.strip():
        html_source = html_text

    # æŠ½å‡ºãƒœã‚¿ãƒ³
    if "articles" not in st.session_state:
        st.session_state.articles = []

    if st.button("è¨˜äº‹å€™è£œã‚’æŠ½å‡º", type="primary", disabled=not html_source.strip()):
        with st.spinner("BeautifulSoupã§æŠ½å‡ºä¸­..."):
            arts = parse_google_alert_html(html_source)
            st.session_state.articles = arts
        if not st.session_state.articles:
            st.warning("è¨˜äº‹å€™è£œã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚HTMLã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else:
            st.success(f"{len(st.session_state.articles)} ä»¶ã®å€™è£œã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚ä¸Šã®ã€â‘¡ æŠ½å‡ºãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€ã‚¿ãƒ–ã¸ã€‚")

with tab_preview:
    st.subheader("æŠ½å‡ºãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
    articles = st.session_state.get("articles", [])
    if not articles:
        st.info("ã¾ã è¨˜äº‹å€™è£œãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã€â‘  å…¥åŠ›ã€ã§HTMLã‚’èª­ã¿è¾¼ã¿ã€ã€è¨˜äº‹å€™è£œã‚’æŠ½å‡ºã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    else:
        for i, a in enumerate(articles):
            with st.container(border=True):
                st.markdown(f"**[{i}] {a['title']}**")
                cols = st.columns([2, 3])
                with cols[0]:
                    st.caption(a.get("source") or "")
                    st.code(a.get("url", ""), language=None)
                with cols[1]:
                    st.write(a.get("snippet") or "")

with tab_output:
    st.subheader("ãƒ‰ãƒ©ãƒ•ãƒˆç”Ÿæˆ")
    articles = st.session_state.get("articles", [])
    if not articles:
        st.info("ã¾ãšã¯è¨˜äº‹å€™è£œã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚")
    else:
        if st.button("LLMã§ã€æœ€ã‚‚è‰¯ã•ãã†ãªè¨˜äº‹ã€ã‚’é¸ã³ã€æ—¥è‹±ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ç”Ÿæˆã™ã‚‹", type="primary"):
            client = get_client()
            with st.spinner("è¨˜äº‹ã‚’è©•ä¾¡ãƒ»é¸å®šã—ã¦ã„ã¾ã™..."):
                picked = rate_and_pick_article(client, articles, model=model)
                idx = picked.get("picked_index", 0)
                best = articles[idx]

            # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°
            h_en = picked.get("hashtags_en", "") if add_hashtags else ""
            h_ja = picked.get("hashtags_ja", "") if add_hashtags else ""

            # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ
            kp_en = picked.get("key_points_en", best["title"])
            kp_ja = picked.get("key_points_ja", best["title"])

            # æ—¥æœ¬èªãƒ‰ãƒ©ãƒ•ãƒˆï¼ˆãƒªãƒ³ã‚¯ç„¡ã—ï¼‰
            ja_post = build_ja_post(
                title=best["title"],
                source=best.get("source", ""),
                key_points=kp_ja,
                hashtag_hint=h_ja if h_ja else None,
            )
            # è‹±èªãƒ‰ãƒ©ãƒ•ãƒˆï¼ˆãƒªãƒ³ã‚¯ä»˜ãï¼‰
            en_post = build_en_post(
                title=best["title"],
                url=best["url"],
                source=best.get("source", ""),
                key_points=kp_en,
                hashtag_hint=h_en if h_en else None,
            )

            st.success("ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")

            st.markdown("#### ğŸ† é¸å®šçµæœï¼ˆè¦ç´„ï¼‰")
            with st.container(border=True):
                st.markdown(f"**ã‚¿ã‚¤ãƒˆãƒ«**: {best['title']}")
                st.caption(best.get("source") or "")
                st.code(best["url"], language=None)
                st.write(f"**LLMç†ç”±**: {picked.get('reason','')}")
                st.write(f"**EN Key Points**: {kp_en}")
                st.write(f"**JA Key Points**: {kp_ja}")

            st.markdown("#### ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªãƒ‰ãƒ©ãƒ•ãƒˆï¼ˆ280æ–‡å­—ä»¥å†…ãƒ»ãƒªãƒ³ã‚¯ãªã—ï¼‰")
            st.text_area("Japanese Draft", value=ja_post, height=120)
            st.caption(f"æ–‡å­—æ•°: {len(ja_post)} / 280")

            st.markdown("#### ğŸ‡ºğŸ‡¸ è‹±èªãƒ‰ãƒ©ãƒ•ãƒˆï¼ˆ280æ–‡å­—ä»¥å†…ãƒ»ãƒªãƒ³ã‚¯ä»˜ãï¼‰")
            st.text_area("English Draft", value=en_post, height=120)
            st.caption(f"æ–‡å­—æ•°: {len(en_post)} / 280")

            # ã¾ã¨ã‚ã¦ JSON ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆä»»æ„ï¼‰
            payload = {
                "picked_index": idx,
                "picked_article": best,
                "reason": picked.get("reason", ""),
                "drafts": {"ja": ja_post, "en": en_post},
                "key_points": {"ja": kp_ja, "en": kp_en},
                "hashtags": {"ja": h_ja, "en": h_en},
                "generated_at": int(time.time()),
            }
            st.download_button(
                "çµæœã‚’JSONã§ä¿å­˜",
                data=json.dumps(payload, ensure_ascii=False, indent=2),
                file_name="x_drafts.json",
                mime="application/json",
            )

# ãƒ•ãƒƒã‚¿
st.markdown("---")
st.caption("Â© X News Draft App â€” parses Google Alerts HTML with BeautifulSoup and drafts bilingual X posts.")
